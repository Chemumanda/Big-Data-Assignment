{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Column functions\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    length,\n",
    "    from_unixtime,\n",
    "    regexp_replace,\n",
    "    lower\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsSentimentAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ad041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning pipeline\n",
    "def clean_review_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning for Amazon reviews\n",
    "    \"\"\"\n",
    "    # Remove null review texts and invalid ratings\n",
    "    cleaned_df = df.filter(\n",
    "        (col(\"reviewText\").isNotNull()) & \n",
    "        (col(\"reviewText\") != \"\") &\n",
    "        (col(\"overall\").between(1, 5))\n",
    "    )\n",
    "    \n",
    "    # Add derived features\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        \"review_length\", length(col(\"reviewText\"))\n",
    "    ).withColumn(\n",
    "        \"review_date\", from_unixtime(col(\"unixReviewTime\"))\n",
    "    ).withColumn(\n",
    "        \"sentiment_label\", \n",
    "        when(col(\"overall\") >= 4, \"positive\")\n",
    "        .when(col(\"overall\") <= 2, \"negative\")\n",
    "        .otherwise(\"neutral\")\n",
    "    ).withColumn(\n",
    "        \"helpful_ratio\",\n",
    "        when(col(\"helpful\").getItem(1) > 0, \n",
    "             col(\"helpful\").getItem(0) / col(\"helpful\").getItem(1))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    # Text preprocessing\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        \"cleaned_text\",\n",
    "        regexp_replace(\n",
    "            regexp_replace(lower(col(\"reviewText\")), \"[^a-zA-Z0-9\\\\s]\", \"\"),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return cleaned_df\n",
    "    \n",
    "# Load raw reviews data\n",
    "raw_reviews = spark.read.json(\"hdfs://namenode:9000/user/data/amazon_reviews/\")\n",
    "\n",
    "# Apply cleaning pipeline\n",
    "processed_reviews = clean_review_data(raw_reviews)\n",
    "\n",
    "# Cache for performance\n",
    "processed_reviews.cache()\n",
    "\n",
    "print(f\"Records after cleaning: {processed_reviews.count()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
